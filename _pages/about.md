---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>


# ğŸ˜Š About Me
Iâ€™m a second-year graduate student at [AIM3 Lab](https://www.ruc-aim3.com/AIM3-Lab.html), Renmin University of China, under the supervision of [Prof. Qin Jin](http://jin-qin.com/). Before that, I received my Bachelorâ€™s degree in 2021 from South China University of Technology. Right now I focus on  <span style="color: red;">I'm looking for Ph.D. student positions for 2024 Fall.</span>


My research interest includes computer vision and multi-modal learning. I have published 2 papers at the top international AI conferences. Here is my google scholar <a href='https://scholar.google.com/citations?user=X-OlO2gAAAAJ&hl=en'>page</a>.


# ğŸ”¥ News
- *2022.11*: &nbsp;ğŸ‰ğŸ‰ One paper is accepted by AAAI 2023! 
- *2022.10*: &nbsp;ğŸ‰ğŸ‰ Our team rank the 1st in Trecvid 2022 VTT task! 
- *2022.05*: &nbsp;ğŸ‰ğŸ‰ One paper is accepted by ECCV 2023!  

# ğŸ“ Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2023</div><img src='images/TOKEN_MIX.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Token Mixing: Parameter-Efficient Transfer Learning from Image-Language to Video-Language](https://github.com/yuqi657/video_language_model/releases/download/v0.1/token_mix.pdf)

**Yuqi Liu***, Luhui Xu, Pengfei Xiong, Qin Jin

[**Project Page**](https://github.com/yuqi657/video_language_model) 
- We study how to transfer knowledge from image-language model to video-language tasks. 
- We have implemented several components proposed by recent works.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Trecvid VTT 2022</div><img src='images/TRECVID_VTT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[TRECVID 2022 task: Video to Text Description
](https://www-nlpir.nist.gov/projects/tvpubs/tv22.papers/rucaim3-tencent.pdf)

Zihao Yue, **Yuqi Liu***, Liang Zhang, Linli Yao, Qin Jin

[**Project Page**](https://github.com/yuezih/BLIP4video)
- We leverage a vision-language pre-training model pre-trained on large-scale image-text datasets for video captioning.
- Our submission ranks 1st in all official evaluation metrics.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2022</div><img src='images/TS2_NET.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[TS2-Net: Token Shift and Selection Transformer for Text-Video Retrieval](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136740311.pdf) 

**Yuqi Liu***, Pengfei Xiong, Luhui Xu, Shengming Cao, Qin Jin

[**Project Page**](https://github.com/yuqi657/ts2_net) 
- TS2-Net is a text-video retrieval model based on [CLIP](https://github.com/openai/CLIP). 
- We propose our token shift transformer and token selection transformer.
</div>
</div>


# ğŸ– Honors and Awards
- *2019.11* **National Scholarship** (Undergraduate).
- *2018.11* **National Scholarship** (Undergraduate).

# ğŸ“– Educations
- *2021.09 - 2024.06 (now)*, M.Phil., School of Information, Remin University of China. 
- *2017.09 - 2021.06*, B.E., School of Software Engineering, South China University of Technology.

# ğŸ’» Internships
- *2022.01 - 2022.10*, Group of video computing, Tencent, China.
- *2020.04 - 2020.10*, Group of wechatPay HK, Tencent, China.